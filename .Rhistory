library(gam)
fit.lr<-loess(mpg~weight+horsepower,data=Auto, subset=train)
lr.pred<-predict(fit.lr,newdata=Auto.test)
unname(lr.pred)
mpg.test
mean(((lr.pred)-mpg.test)^2)
library(gam)
fit.lr<-loess(mpg~weight+horsepower,data=Auto, subset=train)
lr.pred<-predict(fit.lr,newdata=Auto.test)
mean((na.omit(unname((lr.pred)))-mpg.test)^2)
Auto$horsepower_cut <- cut(horsepower[train], 8)
Auto$weight_cut <- cut(weight[train], 8)
fit.step <- glm(mpg ~ horsepower_cut+weight_cut, data=Auto, subset=train)
step.pred <- predict(fit.step,newx=Auto.test)
mean((step.pred-mpg.test)^2)
knitr::opts_chunk$set(echo = TRUE)
x <- c(1,2,3,4,5)
y <- c("yes","no","no","yes","yes")
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce, gini, cross)
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
plot(x, gini)
plot(x, cross)
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini)
points(cross)
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
library(pls)
install.packages("pls")
library(pls)
view(Hitters)
library(pls)
Hitters
library(pls)
na.omit(Hitters)
Hitters
library(pls)
Hitters[!complete.cases(Hitters),]
library(pls)
Hitters <- Hitters[-!complete.cases(Hitters),]
library(pls)
Hitters <- Hitters[-!complete.cases(Hitters),]
Hitters[!complete.cases(Hitters),]
library(pls)
Hitters <- Hitters[!complete.cases(Hitters),]
Hitters <- na.omit(Hitters)
library(pls)
Hitters <- Hitters[!complete.cases(Hitters),]
Hitters <- na.omit(Hitters)
View(Hitters)
library(pls)
Hitters[!complete.cases(Hitters),]
Hitters <- na.omit(Hitters)
View(Hitters)
library(pls)
Hitters[!complete.cases(Hitters),]
Hitters <- na.omit(Hitters)
library(pls)
Hitters[!complete.cases(Hitters),]
na.omit(Hitters)
library(ISLR)
library(pls)
na.omit(Hitters)
library(ISLR)
library(pls)
na.omit(Hitters)
Hitters[,Salary] <- Hitters[,log(Salary)]
View(Hitters)
library(ISLR)
library(pls)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(ISLR)
library(pls)
attach(Hitters)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
install.packages("gbm")
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector(3)
install.packages("gbm")
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector(3)
install.packages("gbm")
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector(3)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,depth=4, shrinkage=skrinkage[i],verbose=F)
train.boost=predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train)))
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,depth=4, shrinkage=skrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=skrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train))
}
plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- NULL
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse, mean(train.boost-Hitters.train))
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse[i] <- mean(train.boost-Hitters.train)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse[i] <- (mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse[i] <- (mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse[i] <- (mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse[i] <- (mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
shrinkage[1]
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-train.boost)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[,-train], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitter, subset=train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[,train], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, n.trees=1000,distribution="gaussian",interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters,n.trees=1000,))^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, subset=train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
training_mse <- c(training_mse,mean((yhat.boost-Hitters.test)^2))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
training_mse <- c(training_mse,mean((yhat.boost-Hitters.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters[-train,], n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Hitters.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
View(Hitters.test)
View(Hitters.train)
library(ISLR)
library(pls)
attach(Hitters)
set.seed(1)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.hitters$train.error))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, training_mse)
lm.hitters <- lm(Salary~., data=Hitters.train)
pred.hitters <- predict(lm.hitters, newdata=Hitters.test)
(mean(pred.hitters-Salary.test)^2)
x<-model.matrix(Salary~.,Hitters)[,-1]
y<-Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-Salary.test)^2)
summary(boost.Hitters)
View(Hitters)
install.packages("randomForest")
library(randomForest)
#set.seed(1)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salaryâˆ¼.,data=Hitters,subset=train,mtry=13,importance =TRUE)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters[-train ,])
plot(yhat.bag, Hitters.test)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
plot(yhat.bag, Hitters.test)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
mean((yhat.bag-Salary.test)^2)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(pls)
attach(Hitters)
set.seed(1)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
