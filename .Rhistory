shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,(mean(yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
View(Hitters.train)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost=predict(boost.Hitters,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,1)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1,2,3,4,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
library(ISLR)
library(pls)
set.seed(1)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
lm.hitters <- lm(Salary~., data=Hitters.train)
pred.hitters <- predict(lm.hitters, newdata=Hitters.test)
(mean(pred.hitters-Salary.test)^2)
x<-model.matrix(Salary~.,Hitters)[,-1]
y<-Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-Salary.test)^2)
summary(boost.Hitters)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
mean((yhat.bag-Salary.test)^2)
knitr::opts_chunk$set(echo = TRUE)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
set.seed(1)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x],verbose=F)
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x],verbose=F)
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test))
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test))
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, test_mse)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters1<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters1,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse[1]
test_mse[2]
test_mse[3]
plot(shrinkage, test_mse)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (x in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[x])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse[x] <- mean((yhat.boost-Salary.test)^2)
}
test_mse
shrinkage <- c(0.001, 0.02, 0.5)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.001)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse1 <- mean((yhat.boost-Salary.test)^2)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.02)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse2 <- mean((yhat.boost-Salary.test)^2)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.5)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse3 <- mean((yhat.boost-Salary.test)^2)
test_mse <- c(test_mse1, test_mse2, test_mse3)
test_mse
shrinkage <- c(0.001, 0.02, 0.5)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.001)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse1 <- mean((yhat.boost-Salary.test)^2)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.02)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse2 <- mean((yhat.boost-Salary.test)^2)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=0.5)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse3 <- mean((yhat.boost-Salary.test)^2)
testm <- c(test_mse1, test_mse2, test_mse3)
testm
knitr::opts_chunk$set(echo = TRUE)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Hitters.test)^2))
}
knitr::opts_chunk$set(echo = TRUE)
# load library
library(tidyverse)
# load in large sample dataset
sample_data <- read_csv("ratings_1_sample.csv")
# Sample only 500 movies.
# extract movieID column
movieID <- as.vector(sample_data$MovieID)
# get only unique movie IDs
unique_movieID <- unique(movieID)
# sample 500 unique movie IDs
set.seed(1)
movieID_sample <- sample(unique_movieID,500, replace=F)
# new data frame, remove and store rows with movie ID in movieID_sample
sample_data2 <- sample_data[ sample_data$MovieID %in% movieID_sample, ]
# check number of unique movie IDs (should be 500)
unique_movieID2 <- unique(sample_data2$MovieID)
# Lastly, I'm going to drop the Date row for the time being.
ratings <- subset(sample_data2, select = c("X1","CustomerID","Rating","MovieID","MovieTitle"))
# Convert all 1 and 2 ratings to 0 and convert all 3, 4, and 5 ratings to 1.
# set minvalue
minval<-4
ratings$Rating[ratings$Rating<minval]=0
ratings$Rating[ratings$Rating>=minval]=1
train <- sample(dim(ratings)[1], dim(ratings)[1]/2)
ratings.train <- ratings[train,]
ratings.test <- ratings[-train,]
rating.test <- ratings$Rating[-train]
# get a feel for the data
summary(ratings)
pairs(subset(sample_data2, select = c("CustomerID","Rating","MovieID")))
cor(subset(sample_data2, select = c("CustomerID","Rating","MovieID")))
# generalized linear model on training data
glm.fit <- glm(Rating~MovieID,data=ratings.train,family=binomial)
glm.probs<-predict(glm.fit,newdata=ratings.test,type="response")
glm.pred<-rep(1,17857)
glm.pred[glm.probs>0.5]=1
mean(glm.pred == rating.test)
library(MASS)
lda.fit<-lda(Rating~MovieID,data=ratings,subset=train)
lda.pred<-predict(lda.fit,ratings.test)
mean(lda.pred$class != rating.test)
qda.fit<-qda(Rating~MovieID,data=ratings,subset=train)
qda.pred<-predict(qda.fit,ratings.test)
mean(qda.pred$class!=rating.test)
library(tree)
Rating1 <- ifelse(ratings$Rating==1,"Yes","No")
ratings<-data.frame(ratings,Rating1)
fit.tree<-tree(Rating1~MovieID,ratings[train,])
summary(fit.tree)
Rating1.test<-ratings$Rating1[-train]
tree.pred<-predict(fit.tree,ratings[-train,],type="class")
table(tree.pred,Rating1.test)
(5165+0)/17857
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
x-matrix(rnorm(20*2),ncol=2)
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
y=c(rep(-1,10),rep(1,10))
x[y==1,]=x[y==1,]+1
plot(x,col=(3-y))
dat=data.frame(x=x,y=as.factor(y)) # encode response as factor
library(e1071)
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
plot(svmfit,dat)
set.seed(1)
tune.out<-tune(svm,y~,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
set.seed(1)
tune.out<-tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
bestmod <- tune.out$best.model
xtest<-matrix(rnorm(20*2),ncol=2)
ytest<-sample(c(-1,1),20,rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat<-data.frame(x=xtest,y=as.factor(ytest))
ypred<-predict(bestmod,testdat)
table(predict=ypred,truth,testdat$y)
xtest<-matrix(rnorm(20*2),ncol=2)
ytest<-sample(c(-1,1),20,rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat<-data.frame(x=xtest,y=as.factor(ytest))
ypred<-predict(bestmod,testdat)
table(predict=ypred,truth=testdat$y)
set.seed(1)
x<-matrix(rnorm(200*2),ncol=2)
x[1:100,]<-x[1:100,]+2
x[101:150,]=x[101:150,]-2
y<-c(rep(1,150),rep(2,50))
dat<-data.frame(x=x,y=as.factor(y))
plot(x,col=y)
dat<-data.frame(x=x,y=as.factor(y)) # encode response as factor
library(e1071)
svmfit<-svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
plot(svmfit,dat)
set.seed(1)
tune.out<-tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
bestmod<-tune.out$best.model
xtest<-matrix(rnorm(20*2),ncol=2)
ytest<-sample(c(-1,1),20,rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat<-data.frame(x=xtest,y=as.factor(ytest))
ypred<-predict(bestmod,testdat)
table(predict=ypred,truth=testdat$y)
train<-sample(200,100)
svmfit<-svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1)
plot(svmfit,dat[train,])
set.seed(1)
tune.out<-tune(svm,y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
table(true=dat[-train,"y"],pred=predict(tune.out$best.model,newx=dat[-train,]))
