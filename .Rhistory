training_mse[i] <- (mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
shrinkage[1]
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(train.boost-Hitters.train)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-train.boost)^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[,-train], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitter, subset=train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[,train], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, n.trees=1000,distribution="gaussian",interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters))^2)
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
#train.boost<-predict(boost.Hitters,newdata=Hitters.train, n.trees=1000)
training_mse <- c(training_mse,mean(Hitters.train-predict(boost.Hitters,n.trees=1000,))^2)
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters, subset=train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
training_mse <- c(training_mse,mean((yhat.boost-Hitters.test)^2))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train, distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
training_mse <- c(training_mse,mean((yhat.boost-Hitters.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters[train,], distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters[-train,], n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Hitters.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
View(Hitters.test)
View(Hitters.train)
library(ISLR)
library(pls)
attach(Hitters)
set.seed(1)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.hitters$train.error))
}
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
#plot(shrinkage, training_mse)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, training_mse)
lm.hitters <- lm(Salary~., data=Hitters.train)
pred.hitters <- predict(lm.hitters, newdata=Hitters.test)
(mean(pred.hitters-Salary.test)^2)
x<-model.matrix(Salary~.,Hitters)[,-1]
y<-Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-Salary.test)^2)
summary(boost.Hitters)
View(Hitters)
install.packages("randomForest")
library(randomForest)
#set.seed(1)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salaryâˆ¼.,data=Hitters,subset=train,mtry=13,importance =TRUE)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters[-train ,])
plot(yhat.bag, Hitters.test)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
plot(yhat.bag, Hitters.test)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
mean((yhat.bag-Salary.test)^2)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(pls)
attach(Hitters)
set.seed(1)
na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
knitr::opts_chunk$set(echo = TRUE)
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean((yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i])
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,(mean(yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,(mean(yhat.boost-Salary.test)^2))
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
View(Hitters.train)
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost=predict(boost.Hitters,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=5000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Hitters.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitterstrain<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitterstrain,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1/5,2/5,3/5,4/5,1)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
#First, I'm going to model the x-axis, plugging in five values for p^m1.
x <- c(1,2,3,4,5)
#The classification error rate is the fraction of training observations in that region that no dont belong to the most common class.
ce <- c(4/5,3/5,2/5,1/5,0)
#The Gini Index measures total variance across the K classes.
gini <- c(0.32,0.48,0.48,0.32,0)
#Cross-entropy
cross <- c(0.500,0.673,0.673,0.500,0)
#The results seem to make sense because the Gini index and cross-entropy are quite similar.
plot(x, ce)
points(gini, col="red")
points(cross, col="blue")
library(ISLR)
library(pls)
set.seed(1)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
library(ISLR)
library(pls)
attach(Hitters)
Hitters <- na.omit(Hitters)
Hitters$Salary <- log(Hitters$Salary)
train <- seq(1,200,1)
test <- seq(201,263,1)
Hitters.train <- Hitters[train,]
Hitters.test <- Hitters[test,]
Salary.test <- Salary[test]
library(gbm)
set.seed(1)
shrinkage <- c(0.001, 0.02, 0.5)
training_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
training_mse <- c(training_mse,mean(boost.Hitters$train.error))
}
training_mse
plot(shrinkage, training_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
#plot(shrinkage, test_mse)
library(gbm)
shrinkage <- c(0.001, 0.02, 0.5)
test_mse <- vector()
for (i in 1:3) {
boost.Hitters<-gbm(Salary~., data=Hitters.train,n.trees=1000,interaction.depth=4,shrinkage=shrinkage[i],verbose=F)
yhat.boost<-predict(boost.Hitters,newdata=Hitters.test, n.trees=1000)
test_mse <- c(test_mse,mean(yhat.boost-Salary.test)^2)
}
test_mse
plot(shrinkage, test_mse)
lm.hitters <- lm(Salary~., data=Hitters.train)
pred.hitters <- predict(lm.hitters, newdata=Hitters.test)
(mean(pred.hitters-Salary.test)^2)
x<-model.matrix(Salary~.,Hitters)[,-1]
y<-Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.pred <- predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-Salary.test)^2)
summary(boost.Hitters)
library(randomForest)
set.seed(1)
bag.Hitters<-randomForest(Salary~.,data=Hitters,subset=train,mtry=13,importance =TRUE)
yhat.bag = predict(bag.Hitters ,newdata=Hitters.test)
mean((yhat.bag-Salary.test)^2)
