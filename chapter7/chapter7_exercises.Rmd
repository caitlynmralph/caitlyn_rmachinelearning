---
title: "Chapter 7 Exercises"
author: "Caitlyn Ralph"
date: "4/10/2018"
output_dir: "."
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Conceptual Exercise

## 5. Consider two curves, 

$g^1$ and $g^2$, defined by

$$\hat{g_1} = argmin_g(\sum_{i=1}^{n}(y_i - g(x_i))^2 + \lambda\int[g^{(3)}(x)]^2dx)$$
$$\hat{g_2} = argmin_g(\sum_{i=1}^{n}(y_i - g(x_i))^2 + \lambda\int[g^{(4)}(x)]^2dx)$$
where $g^{(m)}$ represents the $m$th derivative of $g$.

### (a) As 
$\lambda → \infty$, will $g^1$ or $g^2$ have the smaller training RSS?

The $\lambda$ controls the bias-variance trade-off in a smoothing spline. As lambda decreases, the spline will exactly fit the training data. Therefore, lower values of $\lambda$ create a more flexible model with higher variance and lower bias because there is less of an effect on the loss function $g$. A more intermediate value will choose the appropriate training observations while also not performing terribly on the test data. However, in this case, we are comparing the third derivative in the penalty term to the fourth derivative in the penalty term.

However, the third derivative will make the function stay continuous, but the fourth derivative will cause the tuning parameter to become zero. As aforementioned, if the tuning parameter is zero, then the loss function $g$ will be allowed to perfectly fit to the training observations. Therefore, $g^2$ will have a lower training RSS.

### (b) As 
$\lambda → \infty$, will $g^1$ or $g^2$ have the smaller test RSS?

Similarly, because the tuning parameter controls the loss function from overfitting the data, $g^1$ will have a smaller test RSS.

### (c)  As 
$\lambda → 0$, will $g^1$ or $g^2$ have the smaller training and test RSS?

However, when the tuning parameter becomes zero, then the difference between $g^1$ and $g^2$ doesn't matter. The entire penalty term goes to zero in both cases, causing the test RSS to be larger than the training RSS in both cases since the loss function is allowed to overfit the data.

# Applied Exercise

## 8. Fit some of the non-linear models investigated in this chapter to the `Auto` data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.

```{r}
library(ISLR)
attach(Auto)
summary(Auto)
```

Polynomial Regression and Step Functions:

Fourth-degree polynomial to predict mpg using cylinders

```{r}
fit <- lm(mpg~poly(cylinders,4,raw=T),data=Auto)
coef(summary(fit))
```
```{r}
cyllims <- range(cylinders)
cylinders.grid <- seq(from=cyllims[1],to=cyllims[2])
preds <- predict(fit,newdata=list(cylinders=cylinders.grid),se=TRUE)
se.bands <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(cylinders,mpg,xlim=cyllims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(cylinders.grid,preds$fit,lwd=2,col="blue")
matlines(cylinders.grid,se.bands,lwd=1,col="blue",lty=3)
```
```{r}
fit.1<-lm(mpg~cylinders,data=Auto)
fit.2<-lm(mpg~poly(cylinders,2),data=Auto)
fit.3<-lm(mpg~poly(cylinders,3),data=Auto)
fit.4<-lm(mpg~poly(cylinders,4),data=Auto)
anova(fit.1,fit.2,fit.3,fit.4)
```
Degree-1 or 2 seem to be the best. We can try with some of the other terms.

```{r}
fit.1<-lm(mpg~horsepower+cylinders,data=Auto)
fit.2<-lm(mpg~horsepower+poly(cylinders,2),data=Auto)
fit.3<-lm(mpg~horsepower+poly(cylinders,3),data=Auto)
fit.4<-lm(mpg~horsepower+poly(cylinders,4),data=Auto)
anova(fit.1,fit.2,fit.3,fit.4)
```
This did not appear to help.
```{r}
fit.1<-lm(mpg~horsepower+weight,data=Auto)
fit.2<-lm(mpg~horsepower+poly(weight,2),data=Auto)
fit.3<-lm(mpg~horsepower+poly(weight,3),data=Auto)
fit.4<-lm(mpg~horsepower+poly(weight,4),data=Auto)
anova(fit.1,fit.2,fit.3,fit.4)
```
Ah-ha! The cubic and degree-4 polynomials seem to be the most reasonable here.

Now try to predict whether a vehicle's mpg is greater than 20.
```{r}
horselims <- range(horsepower)
horsepower.grid <- seq(from=horselims[1],to=horselims[2])
fit<-glm(I(mpg>20)~horsepower,data=Auto,family=binomial)
preds<-predict(fit,newdata=list(horsepower=horsepower.grid),se=T)
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))
pred<-predict(fit,newdata=list(horsepower=horsepower.grid),type="response",se=T)
plot(horsepower, I(mpg>20),xlim=horselims,type="n",ylim=c(0,.2))
points(jitter(horsepower),I((mpg>20)/5),cex=.5,pch="|",col="darkgrey")
lines(horsepower.grid,pfit,lwd=2,col="blue")
matlines(horsepower.grid,se.bands,lwd=1,col="blue",lty=3)
```
```{r}
table(cut(horsepower,4))
fit<-lm(mpg~cut(horsepower,4),data=Auto)
coef(summary(fit))
```
Splines:
```{r}
library(splines)
fit<-lm(mpg~bs(horsepower,knots<-c(45,110,160)),data=Auto)
pred<-predict(fit,newdata=list(horsepower=horsepower.grid),se=T)
plot(mpg,horsepower,col="gray")
lines(horsepower.grid,pred$fit,lwd=2)
lines(horsepower.grid,pred$fit+2*pred$se,lty="dashed")
lines(horsepower.grid,pred$fit-2*pred$se,lty="dashed")
```

```{r}
dim(bs(horsepower,knots=c(45,110,160)))
dim(bs(horsepower,df=6))
attr(bs(horsepower,df=6),"knots")
```
Natural Spline
```{r}
fit2<-lm(mpg~ns(horsepower,df=4),data=Auto)
pred2<-predict(fit2,newdata=list(horsepower=horsepower.grid),se=T)
#lines(horsepower.grid,pred2$fit,col="red",lwd=2)
```
Smoothing Spline
```{r}
plot(horsepower,mpg,xlim=horselims,cex=.5,col="darkgrey")
title("Smoothing Spline")
fit<-smooth.spline(horsepower,mpg,df=16)
fit2<-smooth.spline(horsepower,mpg,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),
       col=c("red","blue"),lty=1,lwd=2,cex=.8)
```
Local Regression:
```{r}
plot(horsepower,mpg,xlim=horselims,cex=.5,col="darkgrey")
title("Local Regression")
fit<-loess(mpg~horsepower,span=.2,data=Auto)
fit2<-loess(mpg~horsepower,span=.5,data=Auto)
lines(horsepower.grid,predict(fit,data.frame(horsepower=horsepower.grid)),
      col="red",lwd=2)
lines(horsepower.grid,predict(fit2,data.frame(horsepower=horsepower.grid)),
      col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),
       col=c("red","blue"),lty=1,lwd=2,cex=.8)
```
GAMS:
```{r}
gam1<-lm(mpg~ns(weight,4)+ns(horsepower,5)+cylinders,data=Auto)
library(gam)
gam.m3=gam(mpg~s(weight,4)+s(horsepower,5)+cylinders,data=Auto)
par(mfrow=c(1,3))
plot(gam.m3,se=TRUE,col="blue")
```
```{r}
summary(gam.m3)
```

