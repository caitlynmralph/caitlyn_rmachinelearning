---
title: "Chapter 4: Exercises"
author: "Caitlyn Ralph"
date: "3/5/2018"
# output_dir: "."
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Conceptual Exercises

### 1. Prove that the logistic function representation and logit representations for the logistic regression are equaivalent.

4.2:

\[
p(X) = \frac {e^{\beta_0 + \beta_1X}} {1+e^{\beta_0 + \beta_1X}}
\]

4.3:

\[
\frac {p(X)} {1-p(X)} = e^{\beta_0 + \beta_1X}
\]

First, you plug in 4.2 into 4.3 and get the following: 

\[ 
\frac {\frac {e^{\beta_0 + \beta_1X}} {1 + e^{\beta_0 + \beta_1X}}} {1 - \frac {e^{\beta_0 + \beta_1X}} {1 + e^{\beta_0 + \beta_1X}}}
\]

And then, combine the denominator of the denominator to get:

\[
\frac {\frac {e^{\beta_0 + \beta_1X}} {1 + e^{\beta_0 + \beta_1X}}} {\frac {1} { 1 + e^{\beta_0 + \beta_1X} }}
\]

With some cancellation, you're left simply with:

$$ {e^{\beta_0 + \beta_1X}} $$

### 5. LDA vs. QDA

#### a) If Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?

QDA will perform better on the training set because it is a more flexible algorithm and will fit closely to the data. However, it is prone to overfitting. Because the actual data is linear, LDA will perform better on the test data.

#### b) If the Bayes decision bounary is non-linear, do we expect LDA or QDA to perfom better on the training set? On the test set?

QDA will perform better on both the training and test data because it is a more flexible algorithm.

#### c) In general, as the sample size `n` increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?

QDA performs better than LDA with a higher sample size. As the sample size increases, QDA gets exceedingly more flexible.

#### d)  True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test er- ror rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.

False: If the data is linear, LDA will perform better than QDA.

### 6. Collect data for a group of students in a class. We fit a logistic regression.

Variables:

$$ X_1 = hours studied $$

$$ X_2 = undergrad GPA $$

$$ Y = receive an A $$

Estimated coefficients: 

$$ \hat{\beta}_0 = -6 $$

$$ \hat{\beta}_1 = 0.05 $$
$$ \hat{\beta}_2 = 1 $$

#### a) Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.

\[
\hat{p} (X) = \frac{e^{\hat{\beta} _0 + \hat{\beta} _1X + \hat{\beta} _2X}}{1+e^{\hat{\beta} _0 + \hat{\beta} _1X + \hat{\beta} _2X}}
\]

\[
\hat{p} (X) = \frac{e^{-6 + 0.05 * 40 + 1 * 3.5}}{1+e^{-6 + 0.05 * 40 + 1 * 1}}
\]

\[
\hat{p} (X) = 0.3775
\]

#### b) 

\[
\hat{p} (X) = \frac{e^{-6 + 0.05 * 50 + 1 * 3.5}}{1+e^{-6 + 0.05 * 50 + 1 * 1}}
\]

\[
\hat{p} (X) = 0.5
\]

### 9. Odds

#### a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

$$ \frac {p(X)} {1-p(X)} $$ 

$$ \frac {0.37} {1-0.37} $$ 

#### b) Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

$$ \frac {0.16} {1-0.16} $$ 

## Applied Exercises

### 11. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.

#### a) Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `mpg01` and the other `Auto` variables.

```{r}
library(ISLR)
attach(Auto)
mpg01 = rep(0,length(mpg))
mpg01[mpg>median(mpg)] = 1
Auto_mpg01 = data.frame(Auto,mpg01)
```

#### b) Explore the data graphically in order to investigate the associ- ation between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

```{r}
cor(Auto[,-9])
pairs(Auto)
```

It would seem that `cylinders`, `displacement`, and `weight` are most useful in predicting `mpg01`.

#### c) Split the data into a training set and a test set.

```{r}
train=(year<80)
test = !train
Auto.train=Auto[train,]
Auto.test=Auto[test,]
mpg01.test=mpg01[test]
```

#### d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

```{r}
library(MASS)
lda.fit = lda(mpg01~cylinders+displacement+weight,data=Auto,subset=train)
lda.fit
lda.pred=predict(lda.fit,Auto.test)
mean(lda.pred$class != mpg01.test)
```

The test error rate is 11%.

#### e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

```{r}
qda.fit=qda(mpg01~cylinders+displacement+weight,data=Auto,subset=train)
qda.fit
qda.pred=predict(qda.fit,Auto.test)
mean(qda.pred$class!=mpg01.test)
```
In this case, the test error rate of 13%.

#### f) Perform logistic regression on the training data in order to pre- dict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained? 

```{r}
glm.fit=glm(mpg01~cylinders+displacement+weight,data=Auto,family=binomial,subset=train)
glm.fit
glm.probs=predict(glm.fit,Auto.test,type="response")
glm.pred = rep(0,length(glm.probs))
glm.pred[glm.probs>0.5] = 1
mean(glm.pred!=mpg01.test)
```
In this case, the test error rate is 20%.

#### g) Perform KNN on the training data, with several values of K, in order to predict `mpg01`. Use only the variables that seemed most associated with `mpg01` in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

```{r}
library(class)
train.X = cbind(cylinders,displacement,weight)[train,]
test.X=cbind(cylinders,displacement,weight)[!train,]
train.mpg01 = mpg01[train]
set.seed(1)
knn.pred=knn(train.X,test.X,train.mpg01,k=1)
mean(knn.pred != mpg01.test)
knn.pred=knn(train.X,test.X,train.mpg01,k=10)
mean(knn.pred != mpg01.test)
knn.pred=knn(train.X,test.X,train.mpg01,k=100)
mean(knn.pred != mpg01.test)
```

We should get a different value for each K, but, for some reason, I'm receiving the same.