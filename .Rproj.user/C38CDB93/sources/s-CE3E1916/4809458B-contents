---
title: 'Chapter 6 Exercises: Part 2'
author: "Caitlyn Ralph"
date: "4/1/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Applied Exercises

### 8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

#### (a) Use the `rnorm()` function to generate a predictor X of length n = 100, as well as a noise vector ε of length n = 100.

```{r}
set.seed(1)
X <- rnorm(100)
e <- rnorm(100)
```

#### (b) Generate a response vector Y of length n = 100 according to the model

$$ Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta3X^3 + \epsilon $$
where $\beta_0\text{,}\hspace{0.5cm}\beta_1\text{,}\hspace{0.5cm}\beta_2{,}\hspace{0.5cm}\text{and}\hspace{0.5cm}\beta_3$ are constants of your choice.

I set $\beta_0$ to 10.

I set $\beta_1$ to 15.

I set $\beta_2$ to 20.

I set $\beta_3$ to 30.

```{r}
Y <- 10 + 15*X + 20*X^2 + 30*X^3 + e
```

#### (c) Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors X, X2, . . . , X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the `data.frame()` function to create a single data set containing both X and Y.

```{r}
library(leaps)
d <- data.frame(X,Y)
dim(d)
regfit.full <- regsubsets(Y~poly(X,10,raw=T), d,nvmax=10)
```

```{r} 
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
reg.summary$cp
reg.summary$bic
reg.summary$adjr2
```
The $R^2$ monotonically increases with the number of variables.
```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
points(max_adjr2, reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type="l")
min_cp <- which.min(reg.summary$cp)
points(min_cp, reg.summary$cp[min_cp], col="red",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
```
For Cp, the min is occuring at four variables. For bio, the min is at 4. The max adjusted RSq is at 4 variables.

```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```
It seems the lowest bic and cp are occuring at the 3-variable model.
```{r}
coef(regfit.full,3)
```

#### (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?

Forward stepwise selection:

```{r}
regfit.full <- regsubsets(Y~poly(X,10,raw=T), d,nvmax=10,method="forward")
```

```{r} 
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
reg.summary$cp
reg.summary$bic
reg.summary$adjr2
```

```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
points(max_adjr2, reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type="l")
min_cp <- which.min(reg.summary$cp)
points(min_cp, reg.summary$cp[min_cp], col="red",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
```

```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```

```{r}
coef(regfit.full,3)
```

Backward stepwise selection:

```{r}
regfit.full <- regsubsets(Y~poly(X,10,raw=T), d,nvmax=10,method="backward")
```

```{r} 
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
reg.summary$cp
reg.summary$bic
reg.summary$adjr2
```

```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
points(max_adjr2, reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type="l")
min_cp <- which.min(reg.summary$cp)
points(min_cp, reg.summary$cp[min_cp], col="red",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
```

```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```

```{r}
coef(regfit.full,3)
```
The results from the best subset selection appear to agree with the results from the forward stepwise selection and the backward stepwise selection.

#### (e) Now fit a lasso model to the simulated data, again using X,X2, . . . , X 10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
x<-model.matrix(Y~poly(X,10,raw=T),d)[,-1]
y<-d$Y
```
```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)

set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```
```{r}
lasso.pred <- predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```
```{r}
out <- glmnet(x,y,alpha=1)
predict(out,type="coefficients",s=bestlam)
```
Similar to the results found with subset selection, the lasso selects up to the 3-variable model.

#### (f) Now generate a response vector Y according to the model 

$$ Y = \beta_0 + \beta_7X^7 + \epsilon $$

and perform best subset selection and the lasso. Discuss the results obtained.

I set $\beta_7$ to 25.

```{r}
Y <- 10 + 25*X^7 + e
```
```{r}
library(leaps)
d <- data.frame(X,Y)
dim(d)
regfit.full <- regsubsets(Y~poly(X,10,raw=T), d,nvmax=10)
```
```{r} 
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
reg.summary$cp
reg.summary$bic
reg.summary$adjr2
```
```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
points(max_adjr2, reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)

plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type="l")
min_cp <- which.min(reg.summary$cp)
points(min_cp, reg.summary$cp[min_cp], col="red",cex=2,pch=20)

plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
```
```{r}
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
```
```{r}
coef(regfit.full,2)
```

```{r}
x<-model.matrix(Y~poly(X,10,raw=T),d)[,-1]
y<-d$Y
```
```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)

set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```
```{r}
lasso.pred <- predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```
```{r}
out <- glmnet(x,y,alpha=1)
predict(out,type="coefficients",s=bestlam)
```

As compared to the first part of this question, the results in this section are much harder to interpret. The best subset selection seems to show that the a two-variable model may work well, but the lasso is putting everything to zero except the 7th variable. Using the model with the lower degrees seemed to have better, more consistent results.

### 9. In this exercise, we will predict the number of applications received using the other variables in the `College` data set.

#### (a) Split the data set into a training set and a test set.

```{r}
college <- read.csv("College.csv", header=T, na.strings="?")
rownames(college)=college[,1]
college <- college[,-1]
attach(college)

mean(Apps)
train <- (Apps<3000)
test <- !train
college.train <- college[train,]
college.test <- college[test,]
Apps.test=Apps[test]
```

#### (b) Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
lm.fit=lm(Apps~.,data=college,subset=train)
lm.fit
lm.pred=predict(lm.fit,college.test)
mean(lm.pred != Apps.test)
```

#### (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.

```{r}
x<-model.matrix(Apps~.,college)[,-1]
y<-college$Apps
```
```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)

set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```
```{r}
ridge.pred <- predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
```
```{r}
out <- glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)
```

#### (d) Fit a lasso model on the training set, with λ chosen by cross- validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}
x<-model.matrix(Apps~.,college)[,-1]
y<-college$Apps
```
```{r}
library(glmnet)
grid=10^seq(10,-2,length=100)
lasso.mod=glmnet(x,y,alpha=1,lambda=grid)

set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
test <- (-train)
y.test <- y[test]

set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```
```{r}
lasso.pred <- predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```
```{r}
out <- glmnet(x,y,alpha=1)
predict(out,type="coefficients",s=bestlam)
```

There are 14 non-zero coefficients.

#### (e) Fit a PCR model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
library(pls)
set.seed(2)
pcr.fit <- pcr(Apps~.,data=college,scale=TRUE,validation="CV")
summary(pcr.fit)

set.seed(1)
pcr.fit <- pcr(Apps~., data=college, subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")

pcr.pred <- predict(pcr.fit,x[test,],ncomp=12)
mean((pcr.pred-y.test)^2)

pcr.fit <- pcr(y~x,scale=TRUE,ncomp=12)
summary(pcr.fit)
```

#### (f) Fit a PLS model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
set.seed(1)
pls.fit <- plsr(Apps~., data=college, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)

pls.pred <- predict(pls.fit,x[test,],ncomp=10)
mean((pls.pred-y.test)^2)

pls.fit=plsr(Apps~.,data=college,scale=TRUE,ncomp=10)
summary(pls.fit)
```
#### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

The lasso seems to perform the best because it has the lowest test error (953321.5); however, the difference in test error seems to be quite small.  