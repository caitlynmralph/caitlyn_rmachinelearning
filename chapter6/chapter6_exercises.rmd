---
title: 'Chapter 6: Exercises'
author: "Caitlyn Ralph"
date: "3/25/2018"
output_dir: "."
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Conceptual Exercises

### 1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain `p + 1` models, containing `0, 1, 2, . . . , p` predictors. Explain your answers:

#### (a) Which of the three models with `k` predictors has the smallest training RSS?

The best subset selection model most likely has the smallest training RSS because, by taking into account every single possible model, the best performing model will be found. However, this also has the potential for overfitting.

#### (b) Which of the three models with k predictors has the smallest test RSS?

Similarly, because the best subset selection model is prone to overfitting, its test RSS is most likely large. Therefore, forward stepwise selection or backward stepwise selection will have the smallest RSS.

#### (c) True or False:

##### i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.

True: Forward stepwise selection adds the most useful predictor with each step. Therefore, the k-variable model will be a subset of the k+1-variable model because each step is adding 1. 

##### ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection.

False: Backward stepwise selection iteratively removes the least useful predictor with each step. Therefore, the k-variable model will not be a subset of the k+1-variable model because each step is removing 1. 

##### iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.

True: With each model, backward stepwise selection is removing a least useful predictor. Forward stepwise selection is adding the most useful. Therefore, the k-variable model from backward stepwise selection is a subset of the predictors in the (k+1)-variable model from forward stepwise selection.

##### iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.

False: Because forward stepwise selection is adding the most useful predictor each time, its k variable will not be a subset of the k+1-variable model from backward stepwise selection, which is removing one each time.

##### v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.

False: This is not always true because  best subset selection does not always include the same predictors from the previous step. It chooses the best among all the predictors available.

### 2. For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.

#### (a) The lasso, relative to least squares, is:

iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

#### (b) Ridge regression, relative to least squares, is:

iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Explanation for both (a) and (b): Lasso and ridge regression are both shrinkage methods. Since they impose a penalty on the size of the coefficient, they restrict the values of the coefficient. Therefore, they are less flexible. Less flexible methods have low variance but higher bias, so improved predictive performance happens when the increase in bias is less than the decrease in variance.

#### (c) Non-linear models, relative to least squares, are:

ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Models that are non-linear are more flexible, and they have high variance and low bias. Therefore, improved predictive performance occurs when the increase in variance is less than the decrease in bias.

### 3. Suppose we estimate the regression coefficients in a linear regression model by minimizing

$$ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij}) \text{   subject to   } \sum_{j-1}^{p} |\beta_j|\leq s$$
### for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

#### (a) As we increase `s` from 0, the training RSS will:

iv. Steadily decrease.

Increasing `s` from 0 will select less predictors for the final model, sending more to 0. Therefore, it will fit closer to the training data, leading to a steadily decreasing training RSS.

#### (b) As we increase s from 0, the test RSS will:

ii. Decrease initially, and then eventually start increasing in a U shape.

For the same reasons above, increasing `s` will decrease the test RSS. However, at one point, overfitting occurs, so the test RSS will start to rise, casuing the U shape.

#### (c) As we increase s from 0, the variance will:

iv. Steadily increase

Increasing `s` closer fits to the training data, therefore, variance will steadily increase.

#### (d) As we increase s from 0, the (sqaured) bias will:

iii. Steadily decrease

However, because increasing `s` closer fits to the training data, bias will steadily decrease.

#### (e) As we increase s from 0, the irreducible error will:

v. Remain constant.

The model's flexibility does not affect the irreducible error.

### 4. Suppose we estimate the regression coefficients in a linear regression model by minimizing

$$ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij}) + \sum_{j-1}^{p} \beta_j\leq s$$

### for a particular value of Î». For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.

#### (a) As we increase `s` from 0, the training RSS will:

iv. Steadily decrease

Increasing `s` from 0 will assign a higher penalty to larger coefficients. Therefore, it fits more closely to the data, causing the training RSS to steadily decrease.

#### (b) As we increase s from 0, the test RSS will:

ii. Decrease initially, and then eventually start increasing in a U shape.

For the same reasons above, increasing `s` will decrease the test RSS. However, at one point, overfitting occurs, so the test RSS will start to rise, casuing the U shape.

#### (c) As we increase s from 0, the variance will:

iv. Steadily increase

Increasing `s` closer fits to the training data, therefore, variance will steadily increase.

#### (d) As we increase s from 0, the (sqaured) bias will:

iii. Steadily decrease

However, because increasing `s` closer fits to the training data, bias will steadily decrease.

#### (e) As we increase s from 0, the irreducible error will:

v. Remain constant.

The model's flexibility does not affect the irreducible error.